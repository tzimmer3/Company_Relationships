{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\n",
    "length = 20\n",
    "list_of_strings = []\n",
    "for i in range(0, len(string), length):\n",
    "    list_of_strings.append(string[i:length+i])\n",
    "print(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\n",
    "n = 15\n",
    "\n",
    "def group_words(s, n):\n",
    "    words = s.split()\n",
    "    for i in range(0, len(words), n):\n",
    "        yield ' '.join(words[i:i+n])\n",
    "\n",
    "print(list(group_words(s,n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='when an unknown', metadata={}), Document(page_content='printer took a', metadata={}), Document(page_content='a galley of type and', metadata={}), Document(page_content='and scrambled it to', metadata={}), Document(page_content='it to make a type', metadata={}), Document(page_content='type specimen book.', metadata={}), Document(page_content='I really enjoy', metadata={}), Document(page_content='enjoy eating', metadata={}), Document(page_content='cinnamon rolls.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "s = \"when an unknown printer took a galley of type and scrambled it to make a type specimen book.  \\n I really enjoy eating cinnamon rolls.\"# your text\n",
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "text_splitter = PythonCodeTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 20,\n",
    "    chunk_overlap = 5\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([s])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='when an unknown', metadata={}), Document(page_content='printer took a', metadata={}), Document(page_content='a galley of type and', metadata={}), Document(page_content='and scrambled it to', metadata={}), Document(page_content='it to make a type', metadata={}), Document(page_content='type specimen book.', metadata={}), Document(page_content='I really enjoy', metadata={}), Document(page_content='enjoy eating', metadata={}), Document(page_content='cinnamon rolls.', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "s = \"when an unknown printer took a galley of type and scrambled it to make a type specimen book.  \\n I really enjoy eating cinnamon rolls.\"# your text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size = 20,\n",
    "    chunk_overlap = 5,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([s])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m#import spacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#nlp = spacy.load(\"en_core_web_sm\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_splitter\u001b[39;00m \u001b[39mimport\u001b[39;00m SpacyTextSplitter\n\u001b[1;32m----> 7\u001b[0m text_splitter \u001b[39m=\u001b[39m SpacyTextSplitter(\n\u001b[0;32m      8\u001b[0m     \u001b[39m# Set a really small chunk size, just to show.\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m     separator \u001b[39m=\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     10\u001b[0m     chunk_size \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     chunk_overlap \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m docs \u001b[39m=\u001b[39m text_splitter\u001b[39m.\u001b[39mcreate_documents([s])\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(docs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\nlp_testing\\lib\\site-packages\\langchain\\text_splitter.py:318\u001b[0m, in \u001b[0;36mSpacyTextSplitter.__init__\u001b[1;34m(self, separator, pipeline, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    316\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSpacy is not installed, please install it with `pip install spacy`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m     )\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(pipeline)\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separator \u001b[39m=\u001b[39m separator\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\nlp_testing\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\nlp_testing\\lib\\site-packages\\spacy\\util.py:439\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    438\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "s = \"when an unknown printer took a galley of type and scrambled it to make a type specimen book.  n/ I really enjoy eating cinnamon rolls.\"# your text\n",
    "\n",
    "#import spacy\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    separator = [\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 10\n",
    ")\n",
    "\n",
    "docs = text_splitter.create_documents([s])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
